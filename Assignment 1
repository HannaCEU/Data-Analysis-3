#### SET UP
# CLEAR MEMORY
rm(list=ls())

# Loading the Libraries
library(tidyverse)
library(modelsummary)
library(fixest)
library(kableExtra)
library(data.table)
library(ggplot2)
library(GGally)
library(viridis)
library(caret)
library(grid)
library(dplyr)
library(gridExtra)
library(grid)
library(viridis)
library(lmtest)
library(sandwich)

# Downloading the data
earnings <- read_csv('https://osf.io/4ay9x/download')

# Filter for Lawyers, Judges, Magistrates and other judicial workers 
earnings <- earnings %>% filter(occ2012 == 2100)

# Additional filtering on hours worked per week as at least part time job and a minimum high school diploma
earnings <- earnings %>% filter(uhours>=20 & grade92>=39)

# check out the data summary and variables to model sample
summary(earnings)
datasummary(as.factor(race) ~ N, data = earnings)# white=1, non white > 1
datasummary(as.factor(marital) ~ N, data = earnings)#married and spouse present 1-2

# Creating female and hourly wage variables,age_square, and higher education degrees for complex regression
earnings <- earnings %>% mutate(
  sex= sex - 1, #female is 1, male is 0
  earnhrs = earnwke/uhours,#mutate into hourly wage variable
  ln_earnhrs = log(earnwke/uhours),#mutate into log to decide later if to use
  age_square = age^2, #mutate age as quadratic
  ed_MA=as.numeric(grade92==44),
  ed_Profess = as.numeric(grade92==45),
  ed_PhD = as.numeric(grade92==46))

summary(earnings)

# Model regressions 
m1 <- as.formula(earnhrs ~ sex)
m2 <- as.formula(earnhrs ~ sex + age + age_square)
m3 <- as.formula(earnhrs ~ sex + age + age_square + ed_MA + ed_Profess + ed_PhD)
#for a complex regression
m4 <- as.formula(earnhrs ~ sex + ed_MA + ed_Profess + ed_PhD + age + age_square + sex*ed_MA + sex*ed_Profess + sex*ed_PhD)

reg1<-lm(m1, data = earnings)
reg2<-lm(m2, data = earnings)
reg3<-lm(m3, data = earnings)
reg4<-lm(m4, data = earnings)

model1<-feols(m1, data = earnings, vcov = 'hetero')
model2<-feols(m2, data = earnings, vcov = 'hetero')
model3<-feols(m3, data = earnings, vcov = 'hetero')
model4<-feols(m4, data = earnings, vcov = 'hetero')

regtable<-etable(model1,model2,model3,model4, tex = FALSE)

#regtable
kable(regtable, "latex") %>% column_spec(2:7,width = "5.5em") %>%  
  kable_styling(bootstrap_options = "bordered", full_width = FALSE, repeat_header_text = "Regression table") %>% 
  row_spec(row = 1, bold =TRUE ) %>%
  column_spec(column = 1, width = "6em", bold = TRUE,
              border_left = TRUE) %>%
  column_spec(column = 7, width = "6em",
              border_right = TRUE) 

print(regtable)

###	Compare model performance of these models (a) RMSE in the full sample, (2) cross-validated RMSE and (c) BIC in the full sample. 

##Creating regression comparison
2*(reg1$rank+1) - 2*logLik(reg1)
AIC(reg1)

(reg1$rank+1)*log(nrow(earnings)) - 2*logLik(reg1)
BIC(reg1)

models <- c("reg1", "reg2","reg3", "reg4")
AIC <- c()
BIC <- c()
RMSE <- c()
RSquared <- c()
regr <- c()
k <- c()

for ( i in 1:length(models)){
  AIC[i] <- AIC(get(models[i]))
  BIC[i] <- BIC(get(models[i]))
  RMSE[i] <- RMSE(predict(get(models[i])), get(models[i])$model$earnhrs)
  RSquared[i] <-summary(get(models[i]))$r.squared
  regr[[i]] <- coeftest(get(models[i]), vcov = sandwich)
  k[i] <- get(models[i])$rank -1
}

#Creating comparison data table
eval <- data.frame(models, k, RSquared, RMSE, BIC)

# gsub(pattern, replacement, x) 

eval <- eval %>%
  mutate(models = paste0("(",gsub("reg","",models),")")) %>%
  rename(Model = models, "R-squared" = RSquared, "Training RMSE" = RMSE, "N predictors" = k)

print(eval)

# Simple k-fold cross validation setup:
# set number of folds to use (must be less than the no. observations)
k <- 4

# Model 1
set.seed(1238)
cv1 <- train(m1, earnings, method = "lm", trControl = trainControl(method = "cv", number = k))

# Model 2
set.seed(1238)
cv2 <- train(m2,earnings, method = "lm", trControl = trainControl(method = "cv", number = k))

# Model 3
set.seed(1238)
cv3 <- train(m3, earnings, method = "lm", trControl = trainControl(method = "cv", number = k))

# Model 4
set.seed(1238)
cv4 <- train(m4, earnings, method = "lm", trControl = trainControl(method = "cv", number = k))


# Calculate RMSE for each fold and the average RMSE 
cv <- c("cv1", "cv2", "cv3", "cv4")
rmse_cv <- c()


for(i in 1:length(cv)){
  rmse_cv[i] <- sqrt((get(cv[i])$resample[[1]][1]^2 +
                        get(cv[i])$resample[[1]][2]^2 +
                        get(cv[i])$resample[[1]][3]^2 +
                        get(cv[i])$resample[[1]][4]^2)/4)
}

# summarize results in a table
cv_mat <- data.frame(rbind(cv1$resample[4], "Average"),
                     rbind(cv1$resample[1], rmse_cv[1]),
                     rbind(cv2$resample[1], rmse_cv[2]),
                     rbind(cv3$resample[1], rmse_cv[3]),
                     rbind(cv4$resample[1], rmse_cv[4])
)

colnames(cv_mat)<-c("Resample","Model1", "Model2", "Model3", "Model4")
cv_mat <- cv_mat %>% kbl(caption = "4-Fold Cross Validation and RMSE", booktabs = T) %>% kable_styling(full_width = T)

print(cv_mat)

### Analytical Report
##1.Introduction
#The purpose of this assignment is to create four predictive models for earnings per hour using linear regressions. The source of data can be found [here](https://osf.io/g8p9j/). I chose occupation with code 2100 Lawyers, Judges, Magistrates and other judicial workers which has 1,027 observations. 

##2. Data Analysis and Transformation 
#In the process of data exploration, I carried out data summaries for the data overview. E.g. the sample is varied with a high enough number of women (405) and men (615). Hence, we will use the gender variable for the analysis and carried out respective transformation into a binary. I have also looked into a race variable, however, the number of non-white representatives was rather low, so it was not taken into consideration in the end. The data contains observations for people from 21 years to 64 years old. This may be important in our consideration of the drop in the pay level for older women based on earlier retirement or the education level gap in the older generations. One more variable, quadratic age predictor, is created for our model building. 
#In the data filtering process, education level is included from college to PhD due to the high level of competence required for the job. Only at least 20 hours a week jobs are included to ensure that our comparison is valid. 
#In further munging and transformation, for the target variable I created hourly wage_**(earnhrs)**_ through dividing the weekly earnings (earnwke) by the number of hours (uhours). For additional consideration, I also created the log of this variable _**(ln_earnhrs)**_, which however I dropped after seeing the skewed distribution of the log(see Annex). 

##3. Variables, their interactions, and regression models
#Gender is a strong predictor for earnings per hour. Based on a simple linear regression, on average women in the this professional category earn 4 USD per hour less than men. Further on, we can see in the loess graph in the Annex that older age also leads to an increase in earnings in this professional group. However, it is also interesting to see interaction on gender and age(see Annex), as clearly there is a different dynamic for women in their middle careers (quite likely connected to the maternity leaves and childcare) and further drop closer to 64 years old (possibly connected to an earlier retirement). **Interactions :** To further capture the interplay of independent variables, interactions are used on gender and education to understand the interplay of variables. **Regressions :** Four linear regression models are built to prediction analysis. Model 1 is the simplest containing gender, Model 2 has the Model 1 explanatory variable along with age and age squared. In the Model 3, more explanatory variables on education levels are added. Model 4, is the most complex among as it contains all the mentioned independent variables and the respective interaction on gender and education.
#Model 1 shows an unconditional wage gap with a simple linear regression and heteroskedasticity-robust SE.The coefficient is significant at 0.1% level.It is -4.026, meaning that on average women in the this professional category earns 4 USD less than men per hour. Even when controlling for age, we can see that the Model 2 shows lower earnings for women. In Model 3 uses education level degrees as additional independent variables alongside gender and age. This is clearly an effective factor, as we can see there is a significant increase in R2. In the last model, we use gender and education as interaction coefficient, which shows that income on average is lower to highly educated females as opposed to males who are highly educated. But these findings are not not statistically significant.

##4. Comparison of models performance 
#After building models, Model 4 looks the best positioned based on the lower average RMSE and highest R2. However, there is only insignificant difference with model 3. Most likely, due to the fact that pay level difference within the same higher category of education for both genders is not statistically significant, despite the fact that the combined regression table shows that income on average is still lower to highly educated females as opposed to males who are highly educated. BIC as the measure of fit penalizes the model complexity and helps to avoid over-fitting, so lower BIC models are preferred. During cross-validation, among the models, Model 3 has the lowest BIC and cross validation RMSE average as it is less complex.

#checking out distribution of hourly wages to decide whether to take log wage
# Earnings per hour histogram
hist_w <- ggplot(data=earnings, aes(x=earnhrs)) +
  geom_histogram(aes(y = (..count..)/sum(..count..)), binwidth = 5, boundary=0, fill = "#440154", color = "red", linewidth = 0.2, alpha = 0.5,  show.legend=F, na.rm=TRUE) +
  labs(x = "Earnings per hour",y = "Percent")+
  theme_bw() +
  scale_color_viridis(discrete = TRUE, option = "D") +
  scale_y_continuous(expand = c(0.01,0.01),labels = scales::percent_format(accuracy = 1)) +
  ggtitle("Earnings per hour") 
  
print(hist_w)

# Earnings per hour ln histogram
hist_w_ln <- ggplot(data=earnings, aes(x=ln_earnhrs)) +
  geom_histogram(aes(y = (..count..)/sum(..count..)), boundary=0, fill = "#440154", color = "red", size = 0.2, alpha = 0.5,  show.legend=F, na.rm=TRUE) +
  labs(x = "Earnings per hour Ln (ln(w))",y = "Percent")+
  theme_bw() +
  scale_color_viridis(discrete = TRUE, option = "D") +
  scale_y_continuous(expand = c(0.01,0.01),labels = scales::percent_format(accuracy = 1)) +
  ggtitle("Earnings per hour Ln ") 

print(hist_w_ln)

#First graph has more symmetric distribution and based on that I will be using the simple wage per hour, rather than a log.

# Graph to include earn per hour by gender and age
earnings_mean<- earnings %>%  group_by(sex, age) %>% summarise_at(vars(earnhrs), list(name=mean))
ggplot(earnings_mean)+
  geom_smooth(aes(x=age, y=name, color=as.factor(sex)), method = 'loess')+
  labs(
    title = "Average earning per hour conditional on age by gender",
    y= "Mean earning per hour",
    x= "Age",
    color="Gender")

